<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Lecture 4 Backpropagation and Neural Network | Aung Paing’s Blogs</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lecture 4 Backpropagation and Neural Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction to Neural Network backpropagation flow, equations and intuiation." />
<meta property="og:description" content="Introduction to Neural Network backpropagation flow, equations and intuiation." />
<link rel="canonical" href="https://aungpaing98.github.io/blogs/cs231n-lecture4/" />
<meta property="og:url" content="https://aungpaing98.github.io/blogs/cs231n-lecture4/" />
<meta property="og:site_name" content="Aung Paing’s Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-13T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://aungpaing98.github.io/blogs/cs231n-lecture4/","@type":"BlogPosting","headline":"Lecture 4 Backpropagation and Neural Network","dateModified":"2020-01-13T00:00:00-06:00","datePublished":"2020-01-13T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://aungpaing98.github.io/blogs/cs231n-lecture4/"},"description":"Introduction to Neural Network backpropagation flow, equations and intuiation.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogs/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://aungpaing98.github.io/blogs/feed.xml" title="Aung Paing's Blogs" /><link rel="shortcut icon" type="image/x-icon" href="/blogs/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogs/">Aung Paing&#39;s Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogs/about/">About</a><a class="page-link" href="/blogs/search/">Search</a><a class="page-link" href="/blogs/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 4 | Backpropagation and Neural Network</h1><p class="page-description">Introduction to Neural Network backpropagation flow, equations and intuiation.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-13T00:00:00-06:00" itemprop="datePublished">
        Jan 13, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogs/categories/#computer-vision">computer-vision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#notes">notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#cs231n">cs231n</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#backpropagation">Backpropagation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#local-gradient">Local Gradient</a></li>
<li class="toc-entry toc-h3"><a href="#size-of-gradient">Size of Gradient</a></li>
<li class="toc-entry toc-h3"><a href="#model-architecture">Model Architecture</a></li>
</ul>
</li>
</ul><h2 id="backpropagation">
<a class="anchor" href="#backpropagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backpropagation</h2>

<blockquote>
  <p>recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates
<code class="language-plaintext highlighter-rouge">Lecture Note</code>.</p>
</blockquote>

<p>The basic vanilla model optimization method is called gradient descent, where the gradient of loss respect to weight get descent everytime we update the model parameters. The equation is as follow.
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mi>α</mi><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><msub><mi>W</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">W_{t} = W_{t-1} - \alpha \frac{dL}{dW_{t-1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.366873em;vertical-align:-0.486765em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173142857142857em;"><span style="top:-2.357em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20252142857142857em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.486765em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
In code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># vanilla Graident Descent
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="c1"># loss_fun to calculate loss from data with respect to weigth.
</span>    <span class="n">weight_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> 
    <span class="n">weight</span> <span class="o">+=</span> <span class="o">-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">weight_grad</span>
</code></pre></div></div>
<p>In mathematical term, this is called <em>analytic gradient</em>. They provide an example backpropagation process with simple computational graph.</p>

<p><img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/backprop_simple.png?raw=true" alt="Backpropagation Image" title="Simple Backpropagtion system"></p>

<p>From the graph, we can see the result gradient from two simple operations, addition and multiplication. Addition in local gradient backpropagation will simply return the previous gradient. While in multiplication, local gradient is the same as the other input. So, imagine, normally, we would calculate $dW_{t}$, which will have the graident of $X_{t}*dX_{t+1}$.
If we use <strong>Relu</strong> as the activation function in the intermediate layers:<br></p>

<table>
  <thead>
    <tr>
      <th>$X$</th>
      <th>$W$</th>
      <th>$Relu(W^T X)$</th>
      <th>$dW_{t}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>+</td>
      <td>+</td>
      <td>Positive</td>
      <td>1</td>
    </tr>
    <tr>
      <td>+</td>
      <td>-</td>
      <td>Negative</td>
      <td>0</td>
    </tr>
    <tr>
      <td>-</td>
      <td>+</td>
      <td>Negative</td>
      <td>0</td>
    </tr>
    <tr>
      <td>-</td>
      <td>-</td>
      <td>Positive</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p><code class="language-plaintext highlighter-rouge">1</code> in above table result in $X_t * Relu(W^T X)$ of gradient.</p>

<h3 id="local-gradient">
<a class="anchor" href="#local-gradient" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Gradient</h3>

<p>When the model get too deep, many layers, it is hard to calculate all the computation flow. Instead, we calculate for each local gradient and compile them. Local gradient have the equation of $\frac{d relu(W^T * X)}{dX}$ for each weigth parameter.</p>

<p><img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/local_gradient.png?raw=true" alt="Local Gradient Image" title="Local Gradient"></p>

<p>As mentioned earlier, add gate will result in the previous gradient value, multiplication gate will swap the gradient with another input.So</p>
<ul>
  <li>Add Gate : Gradient Distributor</li>
  <li>Max Gate : Gradient Router</li>
  <li>Mul Gate : Gradient Switcher</li>
</ul>

<h3 id="size-of-gradient">
<a class="anchor" href="#size-of-gradient" aria-hidden="true"><span class="octicon octicon-link"></span></a>Size of Gradient</h3>

<p>The size of the graident is the same or larger than the size of weight parameters. Cause it also have to store the gradient value of the output from the activation function.</p>

<h3 id="model-architecture">
<a class="anchor" href="#model-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Architecture</h3>

<p>In the deep learning papers, we often see the authors describe their layers as 34 layers, 56 layers, and so on, often this refer to how many weight layers exists in the model.</p>

<p><img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/model_layers.png?raw=true" alt="Number of layers in model" title="Numbers of Layers in Model."></p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="aungpaing98/blogs"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogs/cs231n-lecture4/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogs/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogs/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogs/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A Personal Blog Posts</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://www.facebook.com/aung.paing.jj.986" title="aung.paing.jj.986"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#facebook"></use></svg></a></li><li><a rel="me" href="https://github.com/aungpaing98" title="aungpaing98"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/aungpaing98" title="aungpaing98"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
