<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Lecture 6 Training Neural Network 1 | Aung Paing’s Blogs</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lecture 6 Training Neural Network 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Basic Training Procedure hyperparameters optimization." />
<meta property="og:description" content="Basic Training Procedure hyperparameters optimization." />
<link rel="canonical" href="https://aungpaing98.github.io/blogs/cs231n-lecture6/" />
<meta property="og:url" content="https://aungpaing98.github.io/blogs/cs231n-lecture6/" />
<meta property="og:site_name" content="Aung Paing’s Blogs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-13T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://aungpaing98.github.io/blogs/cs231n-lecture6/","@type":"BlogPosting","headline":"Lecture 6 Training Neural Network 1","dateModified":"2020-01-13T00:00:00-06:00","datePublished":"2020-01-13T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://aungpaing98.github.io/blogs/cs231n-lecture6/"},"description":"Basic Training Procedure hyperparameters optimization.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogs/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://aungpaing98.github.io/blogs/feed.xml" title="Aung Paing's Blogs" /><link rel="shortcut icon" type="image/x-icon" href="/blogs/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogs/">Aung Paing&#39;s Blogs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogs/about/">About</a><a class="page-link" href="/blogs/search/">Search</a><a class="page-link" href="/blogs/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 6 | Training Neural Network 1</h1><p class="page-description">Basic Training Procedure hyperparameters optimization.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-13T00:00:00-06:00" itemprop="datePublished">
        Jan 13, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogs/categories/#computer-vision">computer-vision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#deep-learning">deep-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#notes">notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogs/categories/#cs231n">cs231n</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#recap">Recap</a></li>
<li class="toc-entry toc-h2"><a href="#training-model">Training Model</a></li>
<li class="toc-entry toc-h2"><a href="#activation-functions">Activation Functions</a>
<ul>
<li class="toc-entry toc-h3"><a href="#sigmoid-function">Sigmoid Function</a></li>
<li class="toc-entry toc-h3"><a href="#relu-rectified-linear-unit">Relu (Rectified Linear Unit)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#data-preprocessing">Data Preprocessing</a></li>
<li class="toc-entry toc-h2"><a href="#weight-initialization">Weight Initialization</a></li>
<li class="toc-entry toc-h2"><a href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-entry toc-h2"><a href="#babysitting-training">Babysitting Training</a></li>
<li class="toc-entry toc-h2"><a href="#hyper-parameters-optimization">Hyper-parameters Optimization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#cross-validation-strategy">Cross Validation Strategy</a></li>
</ul>
</li>
</ul><h2 id="recap">
<a class="anchor" href="#recap" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recap</h2>

<p><img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/recap_1.png?raw=true" alt="Convolution Image" title="Convolution Operation">
In The previous lesson, we introduced the basic operation of convolution. Because the input is normalized between (-1, 1), so the effect of filter on the input will result on activating the area where the input is similar to the filter. That is the effect of convolution.</p>

<h2 id="training-model">
<a class="anchor" href="#training-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Model</h2>

<p>The overall process of getting a DNN model include, collect data, construct model, train model and evaluate model. In the training part, we have to optimize the model to get better performance. The esscent of optimizing the model is update the weight parameter to get lesser loss value. This is done by optimizer. In the last time, we introduced vanilla gradient descent. By the rate of update according to the use of dataset, it can be further named as:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Update per datapoint</th>
      <th>Number of datapoints in dataset</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SGG (Stochastic Gradient Descent</td>
      <td>Update weight for each data point) eg. 1</td>
      <td>100,000</td>
    </tr>
    <tr>
      <td>Mini-Batch GD (Mini-batch Gradient Descent</td>
      <td>Update weight for small batch of data point) eg. 100</td>
      <td>100,000</td>
    </tr>
    <tr>
      <td>GD (Gradient Descent</td>
      <td>Update weight once for all data point) eg. 100,000</td>
      <td>100,000</td>
    </tr>
  </tbody>
</table>

<p>There are many more optimizer nowaday and most popular one would be <strong>Adam</strong> Optimizer, which will be introduced later.</p>

<h2 id="activation-functions">
<a class="anchor" href="#activation-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation Functions</h2>

<h3 id="sigmoid-function">
<a class="anchor" href="#sigmoid-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sigmoid Function</h3>
<p>Sigmoid function, also called Logistic function, have a “S” continuous function with output range from 0 to 1. The equation for the sigmoid function is as following:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
and the derivative(graident) of sigmoid function is:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\sigma(x)[1-\sigma(x)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>
<img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/sigmoid.jpg?raw=true" alt="Sigoid Equation" title="Sigmoid function in graph showing output ranging from 0 to 1">
<img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/sigmoid_diff.jpg?raw=true" alt="Sigoid Differentiation Equation" title="Differtitation of Sigmoid function in graph showing saturation in - and + region."></p>

<p>3 Problems with Sigmoid</p>
<ul>
  <li>Saturated neurons kill the graident.</li>
  <li>Sigmoid output are not zero center.</li>
  <li>exp() function is expensive to calculate.</li>
</ul>

<p>Since sigmoid output are always positive(not zero center), if we concatenate many cells and its input are from another cell output, it will only get positive value as input. That is why sigmoid, not zero center is bad.</p>

<p>Instead, we will use <code class="language-plaintext highlighter-rouge">tanh</code> activation, which have the advantages of zero center and keep range between (-1, 1). But it still get saturated at the end and kill the gradient.</p>

<h3 id="relu-rectified-linear-unit">
<a class="anchor" href="#relu-rectified-linear-unit" aria-hidden="true"><span class="octicon octicon-link"></span></a>Relu (Rectified Linear Unit)</h3>
<p>Most popular activation function now is <code class="language-plaintext highlighter-rouge">Relu</code> activation, which is simple, and not saturate at (+) region. And practice show it converges much faster than other activation functions.</p>

<p>The equation of <code class="language-plaintext highlighter-rouge">Relu</code> is as follow:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if x&gt;0</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if x &lt; 0</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">Relu(x) = \begin{cases} x &amp; \quad \text{if x&gt;0} \\ 0 &amp; \quad \text{if x &lt; 0}\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">u</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if x&gt;0</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if x &lt; 0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
And the differentiation equation is as follow:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if x&gt;0</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if x &lt; 0</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">Relu(x) = \begin{cases} 1 &amp; \quad \text{if x&gt;0} \\ 0 &amp; \quad \text{if x &lt; 0}\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">u</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if x&gt;0</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if x &lt; 0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>
Althought relu also saturate at $-$ size of the output, it is much easier to compute and simple, and it works. So, in the hidden layers of Neural Network, we often use <code class="language-plaintext highlighter-rouge">Relu</code> as the first choise for activation function.</p>

<p>Aside from <code class="language-plaintext highlighter-rouge">Relu</code>, there are many other activation functions:</p>
<ul>
  <li>Leaky Relu  : $f(x) = max(0.01x, x)$</li>
  <li>Exponential Linear Unit</li>
  <li>Maxout Neuron  : $max(w^T_1x + b_1, w^T_2x + b_2)$</li>
</ul>

<h2 id="data-preprocessing">
<a class="anchor" href="#data-preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Preprocessing</h2>

<p>This include Normalization of data, dimemsion reduction with PCA or T-SNEA if needed and other feature engineering method.</p>

<h2 id="weight-initialization">
<a class="anchor" href="#weight-initialization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Initialization</h2>

<p>Q: What will happen if all weight is initialized with 0?<br>
Cause there is loss value, there will the gradient flow. But, casue the input value is all zero, coming from previous input * weight, it the gradient flow will get kill and the model will not get update.</p>

<p><img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/random_ini.png?raw=true" alt="Random Initialization result image" title="Random Initialization with tanh activation in each layer."></p>

<p>The above figure show the output filter from initializing the model with random values of $W = 0.01 * np.random.randn(D, H)$. Since the output value range from -1 to 1 and the weight value is small, the output is getting smaller and smaller everytime. And the result in output getting closer to 0. The main problem is <em>weight initialization</em> value is too small. What happen if we increase that random initialization value $W = 1 * np.random.randn(D, H)$.
<img src="https://github.com/aungpaing98/blogs/blob/master/_posts/resources/large_ini.png?raw=true" alt=""></p>

<p>To solved this, we generally use <code class="language-plaintext highlighter-rouge">Xavier initialization</code> method.</p>

<h2 id="batch-normalization">
<a class="anchor" href="#batch-normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization</h2>

<p>The idea is that we want middle layers in network to get gaussian distribution too, so we will just get output from activation map and normalize its value. And that is called Batch Normalization. It get the following advantages:</p>
<ul>
  <li>Improve gradient flow through the network</li>
  <li>Allow higher learning rate</li>
  <li>Reduce strong dependence on weight initialization</li>
  <li>A little regularization effect.</li>
</ul>

<h2 id="babysitting-training">
<a class="anchor" href="#babysitting-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Babysitting Training</h2>
<p>Tips</p>
<ul>
  <li>Always do sanity check of model architecture before training.</li>
  <li>When first train, train a small portion and make sure the model can overfit. If so, retrain with all dataset.</li>
  <li>When training, set learning rate to low but not too low and observe result.</li>
</ul>

<h2 id="hyper-parameters-optimization">
<a class="anchor" href="#hyper-parameters-optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hyper-parameters Optimization</h2>

<h3 id="cross-validation-strategy">
<a class="anchor" href="#cross-validation-strategy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Validation Strategy</h3>

<p>Coarse -&gt; Fine<br>
First, try just a few epoch with certain hyper parameters, then try another. If the duration of one hyperparameter getting to converge is about 3 times longer, try another one. And it is better to optimize in Log space.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="aungpaing98/blogs"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blogs/cs231n-lecture6/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogs/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogs/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogs/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A Personal Blog Posts</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://www.facebook.com/aung.paing.jj.986" title="aung.paing.jj.986"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#facebook"></use></svg></a></li><li><a rel="me" href="https://github.com/aungpaing98" title="aungpaing98"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/aungpaing98" title="aungpaing98"><svg class="svg-icon grey"><use xlink:href="/blogs/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
